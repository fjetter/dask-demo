{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Demo\n",
    "## Task queuing / worker-saturation / fixed memory scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "import dask\n",
    "print(f\"Coiled {coiled.__version__}\")\n",
    "from distributed import Client\n",
    "print(f\"Dask {dask.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a couple of utility functions\n",
    "see also https://github.com/coiled/coiled-runtime/blob/main/tests/utils_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import distributed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.datasets import timeseries\n",
    "from dask.sizeof import sizeof\n",
    "from dask.utils import format_bytes, parse_bytes\n",
    "\n",
    "\n",
    "def cluster_memory(client: distributed.Client) -> int:\n",
    "    \"Total memory available on the cluster, in bytes\"\n",
    "    return int(\n",
    "        sum(w[\"memory_limit\"] for w in client.scheduler_info()[\"workers\"].values())\n",
    "    )\n",
    "\n",
    "\n",
    "def timeseries_of_size(\n",
    "    target_nbytes: int | str,\n",
    "    *,\n",
    "    start=\"2000-01-01\",\n",
    "    freq=\"1s\",\n",
    "    partition_freq=\"1d\",\n",
    "    dtypes={\"name\": str, \"id\": int, \"x\": float, \"y\": float},\n",
    "    seed=None,\n",
    "    **kwargs,\n",
    ") -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a `dask.demo.timeseries` of a target total size.\n",
    "\n",
    "    Same arguments as `dask.demo.timeseries`, but instead of specifying an ``end`` date,\n",
    "    you specify ``target_nbytes``. The number of partitions is set as necessary to reach\n",
    "    approximately that total dataset size. Note that you control the partition size via\n",
    "    ``freq``, ``partition_freq``, and ``dtypes``.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> timeseries_of_size(\n",
    "    ...     \"1mb\", freq=\"1s\", partition_freq=\"100s\", dtypes={\"x\": float}\n",
    "    ... ).npartitions\n",
    "    278\n",
    "    >>> timeseries_of_size(\n",
    "    ...     \"1mb\", freq=\"1s\", partition_freq=\"100s\", dtypes={i: float for i in range(10)}\n",
    "    ... ).npartitions\n",
    "    93\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The ``target_nbytes`` refers to the amount of RAM the dask DataFrame would use up\n",
    "    across all workers, as many pandas partitions.\n",
    "\n",
    "    This is typically larger than ``df.compute()`` would be as a single pandas\n",
    "    DataFrame. Especially with many partions, there can be significant overhead to\n",
    "    storing all the individual pandas objects.\n",
    "\n",
    "    Additionally, ``target_nbytes`` certainly does not correspond to the size\n",
    "    the dataset would take up on disk (as parquet, csv, etc.).\n",
    "    \"\"\"\n",
    "    if isinstance(target_nbytes, str):\n",
    "        target_nbytes = parse_bytes(target_nbytes)\n",
    "\n",
    "    start_dt = pd.to_datetime(start)\n",
    "    partition_freq_dt = pd.to_timedelta(partition_freq)\n",
    "    example_part = timeseries(\n",
    "        start=start,\n",
    "        end=start_dt + partition_freq_dt,\n",
    "        freq=freq,\n",
    "        partition_freq=partition_freq,\n",
    "        dtypes=dtypes,\n",
    "        seed=seed,\n",
    "        **kwargs,\n",
    "    )\n",
    "    p = example_part.compute(scheduler=\"threads\")\n",
    "    partition_size = sizeof(p)\n",
    "    npartitions = round(target_nbytes / partition_size)\n",
    "    assert npartitions > 0, (\n",
    "        f\"Partition size of {format_bytes(partition_size)} > \"\n",
    "        f\"target size {format_bytes(target_nbytes)}\"\n",
    "    )\n",
    "\n",
    "    ts = timeseries(\n",
    "        start=start,\n",
    "        end=start_dt + partition_freq_dt * npartitions,\n",
    "        freq=freq,\n",
    "        partition_freq=partition_freq,\n",
    "        dtypes=dtypes,\n",
    "        seed=seed,\n",
    "        **kwargs,\n",
    "    )\n",
    "    assert ts.npartitions == npartitions\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coiled import Cluster\n",
    "\n",
    "cluster = Cluster(\n",
    "    name=\"task-queuing-default\",\n",
    "    n_workers=10,\n",
    "    package_sync=True,\n",
    "    shutdown_on_close=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = cluster_memory(client)\n",
    "format_bytes(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Dataframe Align\n",
    "\n",
    "This example defines two dataframes with different numbers of partitions.\n",
    "\n",
    "These dataframes are added and a simple reduction is performed. Since the two dataframes are partitioned differently, this will trigger a repartitioning which makes the graph non-trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# timeseries_of_size uses dask.dataset.timeseries and \n",
    "df = timeseries_of_size(\n",
    "    memory // 2,\n",
    "    start=\"2020-01-01\",\n",
    "    freq=\"600ms\",\n",
    "    partition_freq=\"12h\",\n",
    "    dtypes={i: float for i in range(100)},\n",
    ")\n",
    "\n",
    "df2 = timeseries_of_size(\n",
    "    memory // 4,\n",
    "    start=\"2010-01-01\",\n",
    "    freq=\"600ms\",\n",
    "    partition_freq=\"12h\",\n",
    "    dtypes={i: float for i in range(100)},\n",
    ")\n",
    "\n",
    "final = (df2 - df).mean()  # will be all NaN, just forcing alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the graph. We can see that some root-tasks (i.e. data generators) need to be split up before they can combine with other data producers.\n",
    "\n",
    "Only after this happens, we can subtract them and reduce the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rescale = 500\n",
    "(\n",
    "    (\n",
    "      df2.partitions[:df2.npartitions//rescale] - df.partitions[:df.npartitions//rescale]\n",
    "    )\n",
    "    .mean()\n",
    "    .visualize(optimize_graph=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.dashboard_link)\n",
    "f1 = client.compute(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will finish eventually... but it requires an awful amount of memory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter: worker-saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coiled import Cluster\n",
    "\n",
    "# Start a second one\n",
    "with dask.config.set({\"distributed.scheduler.worker-saturation\": 1.1}):\n",
    "    cluster_ws = Cluster(\n",
    "        name=\"task-queuing-worker-sat\",\n",
    "        n_workers=10,\n",
    "        package_sync=True,\n",
    "        shutdown_on_close=False,\n",
    "    )\n",
    "client_ws = Client(cluster_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the old one\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.dashboard_link)\n",
    "f1 = client.compute(final)\n",
    "print(client_ws.dashboard_link)\n",
    "f2 = client_ws.compute(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Vorticity (Geo science) / Array reductions\n",
    "See https://github.com/dask/distributed/issues/6571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "def vorticity_graph(n):\n",
    "    shape = (5000, 5000, n)\n",
    "\n",
    "    u = da.random.random(shape, chunks=(5000, 5000, 1))\n",
    "    v = da.random.random(shape, chunks=(5000, 5000, 1))\n",
    "\n",
    "    dx = da.random.random((5001, 5000), chunks=(5001, 5000))\n",
    "    dy = da.random.random((5001, 5000), chunks=(5001, 5000))\n",
    "\n",
    "    def pad_rechunk(arr):\n",
    "        padded = da.pad(arr, pad_width=[(0, 1), (0, 0), (0, 0)], mode=\"wrap\")\n",
    "        old_chunks = padded.chunks\n",
    "        new_chunks = list(old_chunks)\n",
    "        new_chunks[0] = 5001\n",
    "        rechunked = da.rechunk(padded, chunks=new_chunks)\n",
    "        return rechunked\n",
    "\n",
    "    up = pad_rechunk(u)\n",
    "    vp = pad_rechunk(v)\n",
    "    result = dx[..., None] * up - dy[..., None] * vp\n",
    "\n",
    "    def arr_to_devnull(arr: da.Array) -> dask.delayed:\n",
    "        \"Simulate storing an array to zarr, without writing anything (just drops every block once it's computed)\"\n",
    "\n",
    "        class _DevNull:\n",
    "            def __setitem__(self, k, v):\n",
    "                pass\n",
    "\n",
    "        return da.store(arr, _DevNull(), lock=False, compute=False)\n",
    "    return arr_to_devnull(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vorticity_graph(2).visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import distributed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.datasets import timeseries\n",
    "from dask.sizeof import sizeof\n",
    "from dask.utils import format_bytes, parse_bytes\n",
    "\n",
    "def scaled_array_shape(\n",
    "    target_nbytes: int | str,\n",
    "    shape: tuple[int | str, ...],\n",
    "    *,\n",
    "    dtype: np.dtype | type = np.dtype(float),\n",
    "    max_error: float = 0.1,\n",
    ") -> tuple[int, ...]:\n",
    "    \"\"\"\n",
    "    Given a shape with free variables in it, generate the shape that results in the target array size.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> scaled_array_shape(1024, (2, \"x\"), dtype=bool)\n",
    "    (2, 512)\n",
    "    >>> scaled_array_shape(2048, (2, \"x\"), dtype=bool)\n",
    "    (2, 1024)\n",
    "    >>> scaled_array_shape(16, (\"x\", \"x\"), dtype=bool)\n",
    "    (4, 4)\n",
    "    >>> scaled_array_shape(256, (\"4x\", \"x\"), dtype=bool)\n",
    "    (32, 8)\n",
    "    >>> scaled_array_shape(\"10kb\", (\"x\", \"1kb\"), dtype=bool)\n",
    "    (10, 1000)\n",
    "    \"\"\"\n",
    "    if isinstance(target_nbytes, str):\n",
    "        target_nbytes = parse_bytes(target_nbytes)\n",
    "\n",
    "    dtype = np.dtype(dtype)\n",
    "    # Given a shape like:\n",
    "    # (10, \"2x\", 3, \"x\", 50)\n",
    "    # We're solving for x in:\n",
    "    # `10 * 2x * 3 * x * 50 * dtype.itemsize == target_nbytes`\n",
    "    # aka:\n",
    "    # `3000x^2 * dtype.itemsize == target_nbytes`\n",
    "    resolved_shape: list[int | None] = []\n",
    "    x_locs_coeffs: list[tuple[int, float]] = []\n",
    "    total_coeff = 1\n",
    "    for i, s in enumerate(shape):\n",
    "        if isinstance(s, str):\n",
    "            if s[-1] == \"x\":\n",
    "                coeff = 1 if len(s) == 1 else float(s[:-1])\n",
    "                assert coeff > 0, coeff\n",
    "                x_locs_coeffs.append((i, coeff))\n",
    "                total_coeff *= coeff\n",
    "                resolved_shape.append(None)\n",
    "                continue\n",
    "            else:\n",
    "                s = parse_bytes(s) // dtype.itemsize\n",
    "\n",
    "        assert s > 0, s\n",
    "        total_coeff *= s\n",
    "        resolved_shape.append(s)\n",
    "\n",
    "    assert x_locs_coeffs, f\"Expected at least 1 `x` value in shape {shape}\"\n",
    "    total_coeff *= dtype.itemsize\n",
    "    x = (target_nbytes / total_coeff) ** (1 / len(x_locs_coeffs))\n",
    "\n",
    "    # Replace `x` values back into shape\n",
    "    for i, coeff in x_locs_coeffs:\n",
    "        assert resolved_shape[i] is None\n",
    "        resolved_shape[i] = round(coeff * x)\n",
    "\n",
    "    final = tuple(s for s in resolved_shape if s is not None)\n",
    "    assert len(final) == len(resolved_shape), resolved_shape\n",
    "\n",
    "    actual_nbytes = np.prod(final) * dtype.itemsize\n",
    "    error = (actual_nbytes - target_nbytes) / actual_nbytes\n",
    "    assert abs(error) < max_error, (error, actual_nbytes, target_nbytes, final)\n",
    "    return final\n",
    "\n",
    "\n",
    "def cluster_memory(client: distributed.Client) -> int:\n",
    "    \"Total memory available on the cluster, in bytes\"\n",
    "    return int(\n",
    "        sum(w[\"memory_limit\"] for w in client.scheduler_info()[\"workers\"].values())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = vorticity_graph(1363)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = client.compute(final)\n",
    "f2 = client_ws.compute(final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ab0418737cb75d445ff1f78899a8d933bbeb64eddf01df3b8444c71482011b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
